{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Assignment 1\n",
    "| Student Name           | Student Id | Course                          |\n",
    "|------------------------|------------|---------------------------------|\n",
    "| Christo Pananjickal Baby | 8989796    | Reinforcement Learning Programming |\n",
    "\n",
    "\n"
   ],
   "id": "2399590d198c0018"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Problem 1\n",
    "\n",
    "In Reinforcement Learning, the key components are\n",
    "1. Environment\n",
    "2. Agent\n",
    "3. State\n",
    "4. Action\n",
    "5. Reward\n",
    "\n",
    "\n",
    "For this problem lets consider a simple robotic arm which has 2 joints, joints 1 and 2 which can rotate 90 and 45 degrees respectively and a gripper which can open and close as given in below diagram.\n",
    "![Robot arm](<images/Robot arm.png>)\n",
    "\n",
    "### Environment\n",
    "\n",
    "The environment is the workspace of the robot arm, where the robot or the agent is interacting with. It can include the 2 joints with their limits, the gripper, the table, the object to be picked, and the target location where the object must be placed. The physical forces such as gravity, friction etc could also be considered in environment.\n",
    "* **Reasoning** - The environment defines the world in which the agent operates. Including the robot hardware limits and physical forces ensures realistic learning and prevents the agent from attempting impossible actions.\n",
    "\n",
    "\n",
    "### Agent\n",
    "The agent is the robotic arm controller (the RL algorithm) that decides what commands to send to the joints and gripper to perform an action.\n",
    "* **Reasoning**: The agent represents the decision-maker in reinforcement learning. It learns from rewards and chooses actions that maximize task success over time.\n",
    "\n",
    "### State(s)\n",
    "The state describes the current situation. It will include joint 1 angle, joint 2 angle, their velocities, gripper status (opened,closed), position of object on table, target position.\n",
    "* **Reasoning**: The state must fully describe both the robot’s mechanical condition and the task progress. Without these, the agent cannot make informed decisions about what to do next.\n",
    "\n",
    "### Action (a)\n",
    "\n",
    "Action is the step or activity done by the agent. In this case, rotating joint 1 and 2(which can be a small positive or negative torque/step), open or close gripper can be the activity.\n",
    "* **Reasoning**: Actions represent the agent’s control over the environment. Choosing joint rotations and gripper commands allows the robot to move smoothly and manipulate the object to achieve the pick-and-place task.\n",
    "\n",
    "### Reward (r)\n",
    "\n",
    "Reward is the feedback from the environment which tells the agent how good or bad was the action done in a given state.In this example, the reward should encourage fast, smooth, and successful pick-and-place of the object and discourage any unwanted dropping or wrong picking of the object. Rewards can be assigned for different situations, for example,\n",
    "* 100 points if object is placed at target correctly (maximum points because the agent has successfully completed the task)\n",
    "* 5 points if object is picked up correctly (few positive points because first step in the procedure was completed successfully)\n",
    "* -1 point for each second the arm spends. (This is to promote movement speed in the learning process).\n",
    "* -10 points if object is dropped anywhere other than correct location. (Negative 10 because the agent failed to complete the task. From this -10, the agent will understand what it just did is not good)\n",
    "* -5 points for unwanted jerky or energy wasting movements. (Due to this -5 points agent will try to avoid any unwanted movements made between the procedure).\n",
    "* -50 penalty if joint angles exceed the given limits or hitting any other objects. (From this heavy -50 points, the agent will learn never go beyond the given threshold angles or never hit any other objects)\n",
    "\n",
    "\n",
    "* **Reasoning**: Positive and negative feedbacks helps the system understand what is it trying to achieve."
   ],
   "id": "5ece2fdeef656836"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "91914221545c93f2"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
