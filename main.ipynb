{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Assignment 1\n",
    "| Student Name           | Student Id | Course                                        |\n",
    "|------------------------|------------|-----------------------------------------------|\n",
    "| Christo Pananjickal Baby | 8989796    | CSCN8020 - Reinforcement Learning Programming |\n",
    "\n",
    "\n"
   ],
   "id": "2399590d198c0018"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Problem 1\n",
    "\n",
    "In Reinforcement Learning, the key components are\n",
    "1. Environment\n",
    "2. Agent\n",
    "3. State\n",
    "4. Action\n",
    "5. Reward\n",
    "\n",
    "\n",
    "For this problem lets consider a simple robotic arm which has 2 joints, joints 1 and 2 which can rotate 90 and 45 degrees respectively and a gripper which can open and close as given in below diagram.\n",
    "![Robot arm](<images/Robot arm.png>)\n",
    "\n",
    "### Environment\n",
    "\n",
    "The environment is the workspace of the robot arm, where the robot or the agent is interacting with. It can include the 2 joints with their limits, the gripper, the table, the object to be picked, and the target location where the object must be placed. The physical forces such as gravity, friction etc could also be considered in environment.\n",
    "* **Reasoning** - The environment defines the world in which the agent operates. Including the robot hardware limits and physical forces ensures realistic learning and prevents the agent from attempting impossible actions.\n",
    "\n",
    "\n",
    "### Agent\n",
    "The agent is the robotic arm controller (the RL algorithm) that decides what commands to send to the joints and gripper to perform an action.\n",
    "* **Reasoning**: The agent represents the decision-maker in reinforcement learning. It learns from rewards and chooses actions that maximize task success over time.\n",
    "\n",
    "### State(s)\n",
    "The state describes the current situation. It will include joint 1 angle, joint 2 angle, their velocities, gripper status (opened,closed), position of object on table, target position.\n",
    "* **Reasoning**: The state must fully describe both the robot’s mechanical condition and the task progress. Without these, the agent cannot make informed decisions about what to do next.\n",
    "\n",
    "### Action (a)\n",
    "\n",
    "Action is the step or activity done by the agent. In this case, rotating joint 1 and 2(which can be a small positive or negative torque/step), open or close gripper can be the activity.\n",
    "* **Reasoning**: Actions represent the agent’s control over the environment. Choosing joint rotations and gripper commands allows the robot to move smoothly and manipulate the object to achieve the pick-and-place task.\n",
    "\n",
    "### Reward (r)\n",
    "\n",
    "Reward is the feedback from the environment which tells the agent how good or bad was the action done in a given state.In this example, the reward should encourage fast, smooth, and successful pick-and-place of the object and discourage any unwanted dropping or wrong picking of the object. Rewards can be assigned for different situations, for example,\n",
    "* 100 points if object is placed at target correctly (maximum points because the agent has successfully completed the task)\n",
    "* 5 points if object is picked up correctly (few positive points because first step in the procedure was completed successfully)\n",
    "* -1 point for each second the arm spends. (This is to promote movement speed in the learning process).\n",
    "* -10 points if object is dropped anywhere other than correct location. (Negative 10 because the agent failed to complete the task. From this -10, the agent will understand what it just did is not good)\n",
    "* -5 points for unwanted jerky or energy wasting movements. (Due to this -5 points agent will try to avoid any unwanted movements made between the procedure).\n",
    "* -50 penalty if joint angles exceed the given limits or hitting any other objects. (From this heavy -50 points, the agent will learn never go beyond the given threshold angles or never hit any other objects)\n",
    "\n",
    "\n",
    "* **Reasoning**: Positive and negative feedbacks helps the system understand what is it trying to achieve."
   ],
   "id": "5ece2fdeef656836"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Problem 2\n",
    "\n",
    "Given details in the problem:\n",
    "\n",
    "* States (S): {s1, s2, s3, s4}\n",
    "* Actions (A): {up, down, left, right}\n",
    "* Rewards: R(s1) = 5, R(s2) = 10, R(s3) = 1, R(s4) = 2\n",
    "* Transitions: deterministic unless hitting a wall (then stays in the same state).\n",
    "* Discount Factor (γ): not given, so let’s assume γ = 1 (since it is the standard for small use cases).\n",
    "* Initial Policy π: π(up|s) = 1, Which means always move up for every state, but since we are doing value iteration, and we need policy improvement we will update the policy.\n",
    "\n",
    "#### Initialize Value Function\n",
    "At the very beginning, we can set all values to zero:\n",
    "V<sub>0</sub>(s1) = 0, V<sub>0</sub>(s2) = 0, V<sub>0</sub>(s3) = 0, V<sub>0</sub>(s4) = 0,\n",
    "\n",
    "\n",
    "#### Value Iteration Update Equation\n",
    "For each state:\n",
    "V<sub>k+1</sub>(s) = R(s) + γ $\\max_{a}$ Σ<sub>s'</sub> P(s'|s,a) V<sub>k</sub>(s') <br>\n",
    "Since our initial policy is to move up for every state, P(s'|s,a) = 1 <br>\n",
    "So, now our equation can be simplified to V<sub>k+1</sub>(s) = R(s) + γ $\\max_{a}$ V<sub>k</sub>(s')<br>\n",
    "Where R(s) is the immediate reward, max V<sub>k</sub>(s') is the best future value\n",
    "\n",
    "\n",
    "### Iteration 1\n",
    "From initial values, V<sub>0</sub>(s)=0, <br>\n",
    "\n",
    "#### * **s1**\n",
    "\n",
    "Actions:\n",
    "  * up -> stays in s1 (wall) -> V=0\n",
    "  * down -> s3 -> V=0\n",
    "  * left -> stays in s1 -> V=0\n",
    "  * right -> s2 -> V=0 <br>\n",
    "Best action value, V = 0 <br>\n",
    "V<sub>1</sub>(s1) = R(s1) + 0 = 5+0 = 5\n",
    "\n",
    "\n",
    "#### * **s2**\n",
    "\n",
    "Actions:\n",
    "  * up -> stays in s2 (wall) -> V=0\n",
    "  * down -> s4 -> V=0\n",
    "  * left -> s1 -> V=0\n",
    "  * right -> stays in s2 (wall) -> V=0 <br>\n",
    "Best action value, V = 0 <br>\n",
    "V<sub>1</sub>(s2) = R(s2) + 0 = 10 + 0 = 10\n",
    "\n",
    "#### * **s3**\n",
    "\n",
    "Actions:\n",
    "  * up -> s1 -> V=0\n",
    "  * down -> stays in s3 (wall) -> V=0\n",
    "  * left -> stays in s3 (wall) -> V=0\n",
    "  * right -> s4 -> V=0 <br>\n",
    "Best action value, V = 0 <br>\n",
    "V<sub>1</sub>(s3) = R(s3) + 0 = 1 + 0 = 1\n",
    "\n",
    "#### * **s4**\n",
    "\n",
    "Actions:\n",
    "  * up -> s2 -> V=0\n",
    "  * down -> stays in s4 (wall) -> V=0\n",
    "  * left -> s3 -> V=0\n",
    "  * right -> stays in s4 (wall) -> V=0 <br>\n",
    "Best action value, V = 0 <br>\n",
    "V<sub>1</sub>(s4) = R(s4) + 0 = 2 + 0 = 2 <br> <br>\n",
    "\n",
    "\n",
    "So after completing iteration 1, we have <br>\n",
    "V<sub>1</sub> = {s1: 5, s2: 10, s3: 1, s4: 2} <br>\n",
    "\n",
    "\n",
    "### Iteration 2\n",
    "Now we can use V1 values,\n",
    "\n",
    "#### * **s1**\n",
    "Actions:\n",
    "\n",
    "  * up -> s1 -> V=5\n",
    "  * down -> s3 -> V=1\n",
    "  * left -> s1 -> V=5\n",
    "  * right -> s2 -> V=10<br>\n",
    "Best action value, V = 10<br>\n",
    "V2(s1) = R(s1) + V = 5+10 = 15\n",
    "#### * **s2**\n",
    "Actions:\n",
    "\n",
    "  * up -> s2 -> V=10\n",
    "  * down -> s4 -> V=2\n",
    "  * left -> s1 -> V=5\n",
    "  * right -> stays in s2 -> V=10<br>\n",
    "Best action value, V = 10<br>\n",
    "V2(s2) = R(s2) + V = 10 + 10 = 20\n",
    "#### * **s3**\n",
    "Actions:\n",
    "\n",
    "  * up -> s1 -> V=5\n",
    "  * down -> s3 -> V=1\n",
    "  * left -> s3 -> V=1\n",
    "  * right -> s4 -> V=2<br>\n",
    "Best action value, V = 5<br>\n",
    "V2(s3) = R(s3) + V = 1 + 5 = 6\n",
    "#### * **s4**\n",
    "Actions:\n",
    "\n",
    "  * up -> s2 -> V=10\n",
    "  * down -> s4 -> V=2\n",
    "  * left -> s3 -> V=1\n",
    "  * right -> stays in s4 -> V=2<br>\n",
    "Best action value, V = 10<br>\n",
    "V2(s4) = R(s4) + V = 2 + 10 = 12\n",
    "\n",
    "So after completing iteration 2, we have\n",
    "V2 = {s1: 15, s2: 20, s3: 6, s4: 12}\n",
    "<br><br>\n",
    "\n",
    "### Policy Improvement after Iteration 2\n",
    "\n",
    "Now we can choose the greedy action (the action that leads to the best next value) for each state:\n",
    "\n",
    "* s1 -> best action = right (to s2)\n",
    "* s2 -> best action = up or right (stays in s2, value 10)\n",
    "* s3 -> best action = up (to s1)\n",
    "* s4 -> best action = up (to s2)\n",
    "\n",
    "So the improved policy is:\n",
    "π = { s1:right, s2:up, s3:up, s4:up }\n",
    "\n",
    "**We could continue iterating until convergence, but after two iterations we already see the  policy getting better and better.**\n",
    "\n"
   ],
   "id": "373d644a84f151f7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Problem 3",
   "id": "bdce63dc29635469"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "91914221545c93f2"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
